"""
Web Application Scanner Module
==============================

Comprehensive web vulnerability scanning capabilities.
"""

import asyncio
import re
import html
from typing import Dict, List, Optional, Any, Set
from dataclasses import dataclass, field
from datetime import datetime
from urllib.parse import urljoin, urlparse, parse_qs, urlencode
from enum import Enum

import httpx
from bs4 import BeautifulSoup
from rich.console import Console
from rich.progress import Progress

from ..config import get_settings


console = Console()


class Severity(Enum):
    """Vulnerability severity levels"""
    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"
    INFO = "info"


class VulnType(Enum):
    """Vulnerability types"""
    XSS = "Cross-Site Scripting (XSS)"
    SQLI = "SQL Injection"
    LFI = "Local File Inclusion"
    RFI = "Remote File Inclusion"
    SSRF = "Server-Side Request Forgery"
    OPEN_REDIRECT = "Open Redirect"
    CSRF = "Cross-Site Request Forgery"
    IDOR = "Insecure Direct Object Reference"
    SENSITIVE_DATA = "Sensitive Data Exposure"
    SECURITY_HEADER = "Missing Security Header"
    INFO_DISCLOSURE = "Information Disclosure"
    DIRECTORY_LISTING = "Directory Listing"
    BACKUP_FILE = "Backup File Found"
    DEBUG_MODE = "Debug Mode Enabled"
    DEFAULT_CREDS = "Default Credentials"
    CORS = "CORS Misconfiguration"


@dataclass
class Vulnerability:
    """Represents a discovered vulnerability"""
    vuln_type: VulnType
    severity: Severity
    url: str
    parameter: str = ""
    payload: str = ""
    evidence: str = ""
    description: str = ""
    remediation: str = ""
    cwe_id: str = ""
    cvss_score: float = 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "type": self.vuln_type.value,
            "severity": self.severity.value,
            "url": self.url,
            "parameter": self.parameter,
            "payload": self.payload,
            "evidence": self.evidence[:500] if self.evidence else "",
            "description": self.description,
            "remediation": self.remediation,
            "cwe_id": self.cwe_id,
            "cvss_score": self.cvss_score,
        }


@dataclass
class WebScanResult:
    """Complete web scan result"""
    target: str
    vulnerabilities: List[Vulnerability] = field(default_factory=list)
    forms_found: int = 0
    links_crawled: int = 0
    technologies: List[str] = field(default_factory=list)
    security_headers: Dict[str, str] = field(default_factory=dict)
    cookies_analysis: List[Dict[str, Any]] = field(default_factory=list)
    scan_time: datetime = field(default_factory=datetime.now)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "target": self.target,
            "vulnerabilities": [v.to_dict() for v in self.vulnerabilities],
            "forms_found": self.forms_found,
            "links_crawled": self.links_crawled,
            "technologies": self.technologies,
            "security_headers": self.security_headers,
            "cookies_analysis": self.cookies_analysis,
            "scan_time": self.scan_time.isoformat(),
            "summary": {
                "total_vulns": len(self.vulnerabilities),
                "critical": sum(1 for v in self.vulnerabilities if v.severity == Severity.CRITICAL),
                "high": sum(1 for v in self.vulnerabilities if v.severity == Severity.HIGH),
                "medium": sum(1 for v in self.vulnerabilities if v.severity == Severity.MEDIUM),
                "low": sum(1 for v in self.vulnerabilities if v.severity == Severity.LOW),
                "info": sum(1 for v in self.vulnerabilities if v.severity == Severity.INFO),
            }
        }


class WebScanner:
    """
    Web application vulnerability scanner
    
    Features:
    - XSS detection
    - SQL injection testing
    - Security header analysis
    - Cookie security analysis
    - Form discovery and testing
    - Technology fingerprinting
    - Directory/file discovery
    """
    
    # XSS payloads
    XSS_PAYLOADS = [
        '<script>alert(1)</script>',
        '"><script>alert(1)</script>',
        "'-alert(1)-'",
        '<img src=x onerror=alert(1)>',
        '"><img src=x onerror=alert(1)>',
        "javascript:alert(1)",
        '<svg onload=alert(1)>',
        "{{constructor.constructor('alert(1)')()}}",
    ]
    
    # SQL injection payloads
    SQLI_PAYLOADS = [
        "'",
        "''",
        "' OR '1'='1",
        "' OR '1'='1'--",
        "' OR '1'='1'/*",
        "1' ORDER BY 1--",
        "1' ORDER BY 10--",
        "' UNION SELECT NULL--",
        "1; DROP TABLE users--",
        "' AND '1'='1",
        "' AND SLEEP(5)--",
        "1' WAITFOR DELAY '0:0:5'--",
    ]
    
    # SQL error patterns
    SQLI_ERRORS = [
        "sql syntax",
        "mysql_fetch",
        "mysqli_",
        "pg_query",
        "sqlite3",
        "ORA-",
        "SQL Server",
        "ODBC",
        "SQLite",
        "PostgreSQL",
        "syntax error",
        "unclosed quotation",
        "quoted string not properly terminated",
    ]
    
    # LFI payloads
    LFI_PAYLOADS = [
        "../../../etc/passwd",
        "....//....//....//etc/passwd",
        "/etc/passwd",
        "..\\..\\..\\windows\\system32\\drivers\\etc\\hosts",
        "php://filter/convert.base64-encode/resource=index.php",
        "file:///etc/passwd",
    ]
    
    # Common sensitive files
    SENSITIVE_FILES = [
        ".git/config",
        ".env",
        ".htaccess",
        "wp-config.php.bak",
        "config.php.bak",
        ".svn/entries",
        "backup.sql",
        "database.sql",
        "phpinfo.php",
        "info.php",
        "server-status",
        ".DS_Store",
        "web.config",
        "crossdomain.xml",
        "robots.txt",
        "sitemap.xml",
    ]
    
    # Security headers to check
    SECURITY_HEADERS = {
        "Strict-Transport-Security": "Missing HSTS header",
        "X-Frame-Options": "Missing X-Frame-Options (clickjacking)",
        "X-Content-Type-Options": "Missing X-Content-Type-Options",
        "X-XSS-Protection": "Missing X-XSS-Protection",
        "Content-Security-Policy": "Missing Content-Security-Policy",
        "Referrer-Policy": "Missing Referrer-Policy",
        "Permissions-Policy": "Missing Permissions-Policy",
    }
    
    def __init__(
        self,
        timeout: float = 30.0,
        max_concurrent: int = 20,
        user_agent: Optional[str] = None
    ):
        self.settings = get_settings()
        self.timeout = timeout
        self.max_concurrent = min(max_concurrent, self.settings.web.max_concurrent)
        self.user_agent = user_agent or self.settings.web.user_agent
        self._visited_urls: Set[str] = set()
        self._forms: List[Dict[str, Any]] = []
        self._vulnerabilities: List[Vulnerability] = []
    
    async def scan(
        self,
        target: str,
        depth: int = 2,
        check_xss: bool = True,
        check_sqli: bool = True,
        check_lfi: bool = True,
        check_headers: bool = True,
        check_sensitive: bool = True,
        crawl: bool = True
    ) -> WebScanResult:
        """
        Perform comprehensive web vulnerability scan
        
        Args:
            target: Target URL
            depth: Crawl depth
            check_xss: Test for XSS
            check_sqli: Test for SQL injection
            check_lfi: Test for LFI
            check_headers: Check security headers
            check_sensitive: Check for sensitive files
            crawl: Crawl the target for URLs
            
        Returns:
            WebScanResult with all findings
        """
        console.print(f"\n[bold cyan]═══ Web Scan: {target} ═══[/bold cyan]\n")
        
        self._visited_urls = set()
        self._forms = []
        self._vulnerabilities = []
        
        # Normalize target URL
        if not target.startswith(("http://", "https://")):
            target = f"https://{target}"
        
        base_url = target
        parsed = urlparse(target)
        
        async with httpx.AsyncClient(
            timeout=self.timeout,
            follow_redirects=True,
            verify=False,
            headers={"User-Agent": self.user_agent}
        ) as client:
            # Initial request
            try:
                response = await client.get(target)
            except Exception as e:
                console.print(f"[red]Failed to connect: {str(e)}[/red]")
                return WebScanResult(target=target)
            
            # Security Headers Check
            security_headers = {}
            if check_headers:
                console.print("[cyan]Checking security headers...[/cyan]")
                self._check_security_headers(target, response.headers)
                security_headers = dict(response.headers)
            
            # Cookie Analysis
            console.print("[cyan]Analyzing cookies...[/cyan]")
            cookies_analysis = self._analyze_cookies(response.cookies)
            
            # Technology Detection
            console.print("[cyan]Detecting technologies...[/cyan]")
            technologies = self._detect_technologies(response)
            
            # Crawl for URLs and Forms
            urls_to_test = {target}
            if crawl:
                console.print(f"[cyan]Crawling (depth={depth})...[/cyan]")
                urls_to_test = await self._crawl(client, target, depth)
            
            # Check for sensitive files
            if check_sensitive:
                console.print("[cyan]Checking for sensitive files...[/cyan]")
                await self._check_sensitive_files(client, base_url)
            
            # Test for vulnerabilities
            if check_xss or check_sqli or check_lfi:
                console.print("[cyan]Testing for vulnerabilities...[/cyan]")
                
                with Progress() as progress:
                    task = progress.add_task(
                        "[cyan]Testing URLs...",
                        total=len(urls_to_test)
                    )
                    
                    semaphore = asyncio.Semaphore(self.max_concurrent)
                    
                    async def test_url(url: str):
                        async with semaphore:
                            if check_xss:
                                await self._test_xss(client, url)
                            if check_sqli:
                                await self._test_sqli(client, url)
                            if check_lfi:
                                await self._test_lfi(client, url)
                            progress.advance(task)
                    
                    tasks = [test_url(url) for url in urls_to_test]
                    await asyncio.gather(*tasks, return_exceptions=True)
        
        result = WebScanResult(
            target=target,
            vulnerabilities=self._vulnerabilities,
            forms_found=len(self._forms),
            links_crawled=len(self._visited_urls),
            technologies=technologies,
            security_headers=security_headers,
            cookies_analysis=cookies_analysis,
        )
        
        # Display summary
        self._display_summary(result)
        
        return result
    
    def _check_security_headers(
        self,
        url: str,
        headers: httpx.Headers
    ):
        """Check for missing security headers"""
        for header, description in self.SECURITY_HEADERS.items():
            if header.lower() not in [h.lower() for h in headers.keys()]:
                self._vulnerabilities.append(Vulnerability(
                    vuln_type=VulnType.SECURITY_HEADER,
                    severity=Severity.LOW if "XSS" not in header else Severity.MEDIUM,
                    url=url,
                    description=description,
                    remediation=f"Add the {header} header to HTTP responses",
                    cwe_id="CWE-693"
                ))
                console.print(f"  [yellow]⚠[/yellow] {description}")
            else:
                console.print(f"  [green]✓[/green] {header} present")
    
    def _analyze_cookies(
        self,
        cookies: httpx.Cookies
    ) -> List[Dict[str, Any]]:
        """Analyze cookie security attributes"""
        analysis = []
        
        for cookie in cookies.jar:
            cookie_info = {
                "name": cookie.name,
                "secure": cookie.secure,
                "httponly": "httponly" in str(cookie).lower(),
                "samesite": "samesite" in str(cookie).lower(),
                "issues": []
            }
            
            if not cookie.secure:
                cookie_info["issues"].append("Missing Secure flag")
            if not cookie_info["httponly"]:
                cookie_info["issues"].append("Missing HttpOnly flag")
            if not cookie_info["samesite"]:
                cookie_info["issues"].append("Missing SameSite attribute")
            
            if cookie_info["issues"]:
                console.print(
                    f"  [yellow]⚠[/yellow] Cookie '{cookie.name}': "
                    f"{', '.join(cookie_info['issues'])}"
                )
            
            analysis.append(cookie_info)
        
        return analysis
    
    def _detect_technologies(
        self,
        response: httpx.Response
    ) -> List[str]:
        """Detect technologies from response"""
        technologies = []
        headers = response.headers
        body = response.text[:10000].lower()
        
        # Server
        server = headers.get("server", "").lower()
        if "nginx" in server:
            technologies.append("Nginx")
        elif "apache" in server:
            technologies.append("Apache")
        elif "iis" in server:
            technologies.append("Microsoft IIS")
        
        # Powered by
        powered_by = headers.get("x-powered-by", "").lower()
        if "php" in powered_by:
            technologies.append("PHP")
        elif "asp.net" in powered_by:
            technologies.append("ASP.NET")
        elif "express" in powered_by:
            technologies.append("Express.js")
        
        # CMS Detection
        if "wp-content" in body or "wordpress" in body:
            technologies.append("WordPress")
        if "drupal" in body or "sites/all" in body:
            technologies.append("Drupal")
        if "joomla" in body:
            technologies.append("Joomla")
        
        # Frameworks
        if "_next" in body or "__next" in body:
            technologies.append("Next.js")
        if "react" in body and "react-dom" in body:
            technologies.append("React")
        if "__vue__" in body or "vue.js" in body:
            technologies.append("Vue.js")
        if "ng-" in body or "angular" in body:
            technologies.append("Angular")
        if "laravel" in body or "csrf_token" in body:
            technologies.append("Laravel")
        if "django" in body or "csrfmiddlewaretoken" in body:
            technologies.append("Django")
        
        # Security
        if "cloudflare" in headers.get("server", "").lower():
            technologies.append("Cloudflare")
        if "__cf" in str(response.cookies):
            technologies.append("Cloudflare")
        
        for tech in technologies:
            console.print(f"  [green]✓[/green] Detected: {tech}")
        
        return list(set(technologies))
    
    async def _crawl(
        self,
        client: httpx.AsyncClient,
        start_url: str,
        max_depth: int
    ) -> Set[str]:
        """Crawl website for URLs"""
        parsed_start = urlparse(start_url)
        base_domain = parsed_start.netloc
        
        to_visit = [(start_url, 0)]
        visited = set()
        found_urls = set()
        
        while to_visit:
            url, depth = to_visit.pop(0)
            
            if url in visited or depth > max_depth:
                continue
            
            visited.add(url)
            
            try:
                response = await client.get(url)
                
                if "text/html" not in response.headers.get("content-type", ""):
                    continue
                
                soup = BeautifulSoup(response.text, "lxml")
                
                # Find links
                for link in soup.find_all("a", href=True):
                    href = link["href"]
                    full_url = urljoin(url, href)
                    parsed = urlparse(full_url)
                    
                    # Only follow same-domain links
                    if parsed.netloc == base_domain:
                        clean_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
                        if parsed.query:
                            clean_url += f"?{parsed.query}"
                        
                        if clean_url not in visited:
                            to_visit.append((clean_url, depth + 1))
                            found_urls.add(clean_url)
                
                # Find forms
                for form in soup.find_all("form"):
                    form_info = {
                        "action": urljoin(url, form.get("action", "")),
                        "method": form.get("method", "get").lower(),
                        "inputs": []
                    }
                    
                    for input_tag in form.find_all(["input", "textarea", "select"]):
                        input_info = {
                            "name": input_tag.get("name", ""),
                            "type": input_tag.get("type", "text"),
                            "value": input_tag.get("value", ""),
                        }
                        if input_info["name"]:
                            form_info["inputs"].append(input_info)
                    
                    if form_info["inputs"]:
                        self._forms.append(form_info)
                        found_urls.add(form_info["action"])
                
            except Exception:
                continue
        
        self._visited_urls = visited
        console.print(f"  [green]Found {len(found_urls)} URLs, {len(self._forms)} forms[/green]")
        
        return found_urls
    
    async def _check_sensitive_files(
        self,
        client: httpx.AsyncClient,
        base_url: str
    ):
        """Check for sensitive files"""
        parsed = urlparse(base_url)
        base = f"{parsed.scheme}://{parsed.netloc}"
        
        for file_path in self.SENSITIVE_FILES:
            url = f"{base}/{file_path}"
            try:
                response = await client.get(url)
                
                if response.status_code == 200:
                    # Check if it's a real file, not a custom 404
                    content = response.text.lower()
                    if "not found" not in content and "error" not in content[:100]:
                        severity = Severity.HIGH if any(
                            x in file_path for x in [".env", ".git", "config", "sql"]
                        ) else Severity.MEDIUM
                        
                        self._vulnerabilities.append(Vulnerability(
                            vuln_type=VulnType.SENSITIVE_DATA,
                            severity=severity,
                            url=url,
                            description=f"Sensitive file accessible: {file_path}",
                            evidence=response.text[:200],
                            remediation="Remove or restrict access to sensitive files",
                            cwe_id="CWE-538"
                        ))
                        console.print(f"  [red]✗[/red] Found: {file_path}")
                        
            except Exception:
                continue
    
    async def _test_xss(
        self,
        client: httpx.AsyncClient,
        url: str
    ):
        """Test URL for XSS vulnerabilities"""
        parsed = urlparse(url)
        params = parse_qs(parsed.query)
        
        if not params:
            return
        
        for param_name, param_values in params.items():
            for payload in self.XSS_PAYLOADS[:3]:  # Test first 3 payloads
                test_params = params.copy()
                test_params[param_name] = [payload]
                
                test_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
                test_url += f"?{urlencode(test_params, doseq=True)}"
                
                try:
                    response = await client.get(test_url)
                    
                    # Check if payload is reflected
                    if payload in response.text:
                        # Check if it's unencoded (potential XSS)
                        if not html.escape(payload) in response.text:
                            self._vulnerabilities.append(Vulnerability(
                                vuln_type=VulnType.XSS,
                                severity=Severity.HIGH,
                                url=url,
                                parameter=param_name,
                                payload=payload,
                                evidence=response.text[
                                    max(0, response.text.find(payload) - 50):
                                    response.text.find(payload) + len(payload) + 50
                                ],
                                description="Reflected XSS vulnerability detected",
                                remediation="Encode all user input before rendering",
                                cwe_id="CWE-79",
                                cvss_score=6.1
                            ))
                            console.print(
                                f"  [red]✗[/red] XSS in {param_name} at {url[:50]}..."
                            )
                            break
                            
                except Exception:
                    continue
    
    async def _test_sqli(
        self,
        client: httpx.AsyncClient,
        url: str
    ):
        """Test URL for SQL injection"""
        parsed = urlparse(url)
        params = parse_qs(parsed.query)
        
        if not params:
            return
        
        # Get baseline response
        try:
            baseline = await client.get(url)
            baseline_length = len(baseline.text)
        except Exception:
            return
        
        for param_name, param_values in params.items():
            for payload in self.SQLI_PAYLOADS[:5]:  # Test first 5 payloads
                test_params = params.copy()
                original_value = param_values[0] if param_values else ""
                test_params[param_name] = [f"{original_value}{payload}"]
                
                test_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
                test_url += f"?{urlencode(test_params, doseq=True)}"
                
                try:
                    response = await client.get(test_url)
                    response_text = response.text.lower()
                    
                    # Check for SQL errors
                    for error in self.SQLI_ERRORS:
                        if error.lower() in response_text:
                            self._vulnerabilities.append(Vulnerability(
                                vuln_type=VulnType.SQLI,
                                severity=Severity.CRITICAL,
                                url=url,
                                parameter=param_name,
                                payload=payload,
                                evidence=response.text[:300],
                                description="SQL Injection vulnerability - error based",
                                remediation="Use parameterized queries and input validation",
                                cwe_id="CWE-89",
                                cvss_score=9.8
                            ))
                            console.print(
                                f"  [red]✗[/red] SQLi in {param_name} at {url[:50]}..."
                            )
                            return
                    
                    # Check for significant response difference (blind SQLi indicator)
                    if abs(len(response.text) - baseline_length) > baseline_length * 0.3:
                        self._vulnerabilities.append(Vulnerability(
                            vuln_type=VulnType.SQLI,
                            severity=Severity.HIGH,
                            url=url,
                            parameter=param_name,
                            payload=payload,
                            description="Possible blind SQL Injection - response differs significantly",
                            remediation="Use parameterized queries and input validation",
                            cwe_id="CWE-89",
                            cvss_score=8.5
                        ))
                        console.print(
                            f"  [yellow]⚠[/yellow] Possible SQLi in {param_name}"
                        )
                        break
                        
                except Exception:
                    continue
    
    async def _test_lfi(
        self,
        client: httpx.AsyncClient,
        url: str
    ):
        """Test URL for Local File Inclusion"""
        parsed = urlparse(url)
        params = parse_qs(parsed.query)
        
        if not params:
            return
        
        # Look for file-related parameters
        file_params = [p for p in params.keys() if any(
            x in p.lower() for x in ["file", "path", "page", "include", "doc", "template"]
        )]
        
        if not file_params:
            return
        
        for param_name in file_params:
            for payload in self.LFI_PAYLOADS:
                test_params = params.copy()
                test_params[param_name] = [payload]
                
                test_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
                test_url += f"?{urlencode(test_params, doseq=True)}"
                
                try:
                    response = await client.get(test_url)
                    
                    # Check for /etc/passwd content
                    if "root:" in response.text or "daemon:" in response.text:
                        self._vulnerabilities.append(Vulnerability(
                            vuln_type=VulnType.LFI,
                            severity=Severity.CRITICAL,
                            url=url,
                            parameter=param_name,
                            payload=payload,
                            evidence=response.text[:300],
                            description="Local File Inclusion vulnerability",
                            remediation="Validate and sanitize file paths, use allowlists",
                            cwe_id="CWE-98",
                            cvss_score=9.1
                        ))
                        console.print(
                            f"  [red]✗[/red] LFI in {param_name} at {url[:50]}..."
                        )
                        return
                    
                    # Check for Windows hosts file
                    if "localhost" in response.text and "#" in response.text:
                        self._vulnerabilities.append(Vulnerability(
                            vuln_type=VulnType.LFI,
                            severity=Severity.CRITICAL,
                            url=url,
                            parameter=param_name,
                            payload=payload,
                            evidence=response.text[:300],
                            description="Local File Inclusion vulnerability (Windows)",
                            remediation="Validate and sanitize file paths",
                            cwe_id="CWE-98",
                            cvss_score=9.1
                        ))
                        console.print(
                            f"  [red]✗[/red] LFI (Windows) in {param_name}"
                        )
                        return
                        
                except Exception:
                    continue
    
    def _display_summary(self, result: WebScanResult):
        """Display scan summary"""
        from rich.table import Table
        
        console.print(f"\n[bold cyan]═══ Web Scan Summary ═══[/bold cyan]")
        
        summary = result.to_dict()["summary"]
        
        table = Table(show_header=True, header_style="bold magenta")
        table.add_column("Metric", style="cyan")
        table.add_column("Value", style="white")
        
        table.add_row("Target", result.target)
        table.add_row("URLs Crawled", str(result.links_crawled))
        table.add_row("Forms Found", str(result.forms_found))
        table.add_row("Technologies", ", ".join(result.technologies) or "Unknown")
        table.add_row(
            "Vulnerabilities",
            f"[red]{summary['critical']}C[/red] | "
            f"[orange1]{summary['high']}H[/orange1] | "
            f"[yellow]{summary['medium']}M[/yellow] | "
            f"[green]{summary['low']}L[/green] | "
            f"[blue]{summary['info']}I[/blue]"
        )
        
        console.print(table)
        
        # Show critical/high vulnerabilities
        critical_high = [
            v for v in result.vulnerabilities
            if v.severity in [Severity.CRITICAL, Severity.HIGH]
        ]
        
        if critical_high:
            console.print(f"\n[bold red]Critical/High Findings:[/bold red]")
            for v in critical_high:
                console.print(
                    f"  • [{v.severity.value.upper()}] {v.vuln_type.value}\n"
                    f"    URL: {v.url[:60]}...\n"
                    f"    Parameter: {v.parameter or 'N/A'}"
                )
