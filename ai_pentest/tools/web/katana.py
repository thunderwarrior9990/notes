"""
Katana Crawler Wrapper
======================

Next-generation crawling and spidering framework.
"""

import json
from typing import Dict, List, Any, Tuple, Optional

from ..base import ToolWrapper


class KatanaCrawler(ToolWrapper):
    """
    Katana - Web crawler
    
    Features:
    - JavaScript rendering
    - Scope control
    - Custom headers
    - Form filling
    """
    
    TOOL_NAME = "Katana"
    BINARY_NAME = "katana"
    INSTALL_HINT = "Install with: go install github.com/projectdiscovery/katana/cmd/katana@latest"
    
    def build_command(
        self,
        target: str,
        depth: int = 3,
        threads: int = 10,
        headless: bool = False,
        js_crawl: bool = True,
        known_files: bool = True,
        form_fill: bool = True,
        scope_filter: Optional[str] = None,
        field_scope: str = "rdn",
        timeout: int = 15,
        delay: int = 0,
        extensions: Optional[List[str]] = None,
        exclude_extensions: Optional[List[str]] = None,
        **kwargs
    ) -> List[str]:
        cmd = [self.BINARY_NAME]
        
        cmd.extend(["-u", target])
        cmd.extend(["-d", str(depth)])
        cmd.extend(["-c", str(threads)])
        cmd.extend(["-timeout", str(timeout)])
        
        if headless:
            cmd.append("-headless")
        
        if js_crawl:
            cmd.append("-js-crawl")
        
        if known_files:
            cmd.append("-kf")
        
        if form_fill:
            cmd.append("-aff")
        
        if scope_filter:
            cmd.extend(["-sf", scope_filter])
        
        cmd.extend(["-fs", field_scope])
        
        if delay > 0:
            cmd.extend(["-delay", str(delay)])
        
        if extensions:
            cmd.extend(["-em", ",".join(extensions)])
        
        if exclude_extensions:
            cmd.extend(["-ef", ",".join(exclude_extensions)])
        
        output_file = self.output_dir / f"katana_{target.replace('://', '_').replace('/', '_')}.json"
        cmd.extend(["-jsonl", "-o", str(output_file)])
        
        return cmd
    
    def parse_output(self, output: str, error: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
        findings = []
        raw_data = {"urls": [], "endpoints": [], "forms": [], "js_files": []}
        
        json_files = list(self.output_dir.glob("katana_*.json"))
        if json_files:
            try:
                with open(json_files[-1]) as f:
                    for line in f:
                        try:
                            entry = json.loads(line.strip())
                            url = entry.get("request", {}).get("endpoint", "")
                            
                            if url:
                                raw_data["urls"].append(url)
                                
                                # Categorize
                                if url.endswith(".js"):
                                    raw_data["js_files"].append(url)
                                elif "?" in url:
                                    raw_data["endpoints"].append(url)
                        except json.JSONDecodeError:
                            continue
            except Exception:
                pass
        
        # Create summary finding
        if raw_data["urls"]:
            findings.append(self._create_finding(
                title=f"Crawled {len(raw_data['urls'])} URLs",
                severity="info",
                description=f"Endpoints: {len(raw_data['endpoints'])}, JS Files: {len(raw_data['js_files'])}",
                url_count=len(raw_data["urls"])
            ))
        
        # Highlight interesting endpoints
        for endpoint in raw_data["endpoints"][:20]:
            if any(x in endpoint.lower() for x in ["api", "admin", "upload", "login", "auth"]):
                findings.append(self._create_finding(
                    title=f"Interesting Endpoint: {endpoint}",
                    severity="low",
                    description="Potentially interesting for testing",
                    url=endpoint
                ))
        
        return findings, raw_data
