"""
Cursor AI Client
================

Client for interacting with Cursor API (Anthropic Claude backend)
for AI-assisted penetration testing analysis.
"""

import asyncio
import json
import os
from typing import Optional, Dict, Any, List, AsyncGenerator
from dataclasses import dataclass, field
from enum import Enum

import httpx
from rich.console import Console

from ..config import get_settings, Settings


console = Console()


class AIProvider(Enum):
    """Supported AI providers"""
    CURSOR = "cursor"
    ANTHROPIC = "anthropic"
    OPENAI_COMPATIBLE = "openai_compatible"


@dataclass
class AIMessage:
    """Represents an AI message"""
    role: str
    content: str


@dataclass
class AIResponse:
    """Represents an AI response"""
    content: str
    model: str
    usage: Dict[str, int] = field(default_factory=dict)
    finish_reason: Optional[str] = None


class CursorAIClient:
    """
    AI Client compatible with Cursor API
    
    Supports multiple backends:
    - Cursor API (default)
    - Direct Anthropic API (fallback)
    - OpenAI-compatible endpoints
    """
    
    def __init__(
        self,
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        model: Optional[str] = None,
        provider: AIProvider = AIProvider.ANTHROPIC,
        settings: Optional[Settings] = None,
    ):
        self.settings = settings or get_settings()
        self.provider = provider
        
        # Determine API configuration based on provider
        if provider == AIProvider.CURSOR:
            self.api_key = api_key or self.settings.cursor.api_key or os.environ.get("CURSOR_API_KEY", "")
            self.base_url = base_url or self.settings.cursor.base_url
            self.model = model or self.settings.cursor.model
            self.max_tokens = self.settings.cursor.max_tokens
            self.temperature = self.settings.cursor.temperature
        else:  # Default to Anthropic
            self.api_key = api_key or self.settings.anthropic.api_key or os.environ.get("ANTHROPIC_API_KEY", "")
            self.base_url = base_url or self.settings.anthropic.base_url
            self.model = model or self.settings.anthropic.model
            self.max_tokens = self.settings.anthropic.max_tokens
            self.temperature = 0.3
        
        self.conversation_history: List[AIMessage] = []
        self._client: Optional[httpx.AsyncClient] = None
    
    async def __aenter__(self):
        await self._init_client()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.close()
    
    async def _init_client(self):
        """Initialize HTTP client"""
        if self._client is None:
            self._client = httpx.AsyncClient(
                timeout=httpx.Timeout(120.0, connect=30.0),
                headers=self._get_headers()
            )
    
    async def close(self):
        """Close the HTTP client"""
        if self._client:
            await self._client.aclose()
            self._client = None
    
    def _get_headers(self) -> Dict[str, str]:
        """Get API headers based on provider"""
        headers = {
            "Content-Type": "application/json",
        }
        
        if self.provider == AIProvider.CURSOR:
            headers["Authorization"] = f"Bearer {self.api_key}"
        elif self.provider == AIProvider.ANTHROPIC:
            headers["x-api-key"] = self.api_key
            headers["anthropic-version"] = "2023-06-01"
        else:
            headers["Authorization"] = f"Bearer {self.api_key}"
        
        return headers
    
    def _build_request_body(
        self,
        messages: List[Dict[str, str]],
        system_prompt: Optional[str] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
    ) -> Dict[str, Any]:
        """Build request body for the API"""
        body: Dict[str, Any] = {
            "model": self.model,
            "max_tokens": max_tokens or self.max_tokens,
        }
        
        if temperature is not None:
            body["temperature"] = temperature
        elif hasattr(self, 'temperature'):
            body["temperature"] = self.temperature
        
        if self.provider == AIProvider.ANTHROPIC:
            if system_prompt:
                body["system"] = system_prompt
            body["messages"] = messages
        else:
            # OpenAI-compatible format
            formatted_messages = []
            if system_prompt:
                formatted_messages.append({"role": "system", "content": system_prompt})
            formatted_messages.extend(messages)
            body["messages"] = formatted_messages
        
        return body
    
    async def analyze(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None,
        temperature: Optional[float] = None,
    ) -> AIResponse:
        """
        Analyze security data using AI
        
        Args:
            prompt: The analysis prompt
            system_prompt: System prompt for context
            context: Additional context data
            temperature: Response temperature (0-1)
        
        Returns:
            AIResponse with analysis results
        """
        await self._init_client()
        
        if not self.api_key:
            return AIResponse(
                content=self._generate_offline_analysis(prompt, context),
                model="offline",
                usage={},
                finish_reason="offline_mode"
            )
        
        # Build the user message
        user_content = prompt
        if context:
            user_content = f"{prompt}\n\nContext:\n```json\n{json.dumps(context, indent=2)}\n```"
        
        messages = [{"role": "user", "content": user_content}]
        
        # Add conversation history if any
        if self.conversation_history:
            history_messages = [
                {"role": msg.role, "content": msg.content}
                for msg in self.conversation_history[-10:]  # Last 10 messages
            ]
            messages = history_messages + messages
        
        body = self._build_request_body(
            messages=messages,
            system_prompt=system_prompt,
            temperature=temperature
        )
        
        try:
            endpoint = f"{self.base_url}/messages"
            response = await self._client.post(endpoint, json=body)
            response.raise_for_status()
            
            data = response.json()
            
            # Parse response based on provider format
            if self.provider == AIProvider.ANTHROPIC:
                content = data.get("content", [{}])[0].get("text", "")
                usage = data.get("usage", {})
            else:
                content = data.get("choices", [{}])[0].get("message", {}).get("content", "")
                usage = data.get("usage", {})
            
            # Update conversation history
            self.conversation_history.append(AIMessage(role="user", content=prompt))
            self.conversation_history.append(AIMessage(role="assistant", content=content))
            
            return AIResponse(
                content=content,
                model=data.get("model", self.model),
                usage=usage,
                finish_reason=data.get("stop_reason", "complete")
            )
            
        except httpx.HTTPStatusError as e:
            console.print(f"[red]API Error: {e.response.status_code}[/red]")
            return AIResponse(
                content=self._generate_offline_analysis(prompt, context),
                model="offline",
                usage={},
                finish_reason="api_error"
            )
        except Exception as e:
            console.print(f"[red]Error: {str(e)}[/red]")
            return AIResponse(
                content=self._generate_offline_analysis(prompt, context),
                model="offline",
                usage={},
                finish_reason="error"
            )
    
    async def stream_analyze(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None,
    ) -> AsyncGenerator[str, None]:
        """
        Stream AI analysis results
        
        Yields chunks of the response as they arrive.
        """
        await self._init_client()
        
        if not self.api_key:
            yield self._generate_offline_analysis(prompt, context)
            return
        
        user_content = prompt
        if context:
            user_content = f"{prompt}\n\nContext:\n```json\n{json.dumps(context, indent=2)}\n```"
        
        messages = [{"role": "user", "content": user_content}]
        
        body = self._build_request_body(
            messages=messages,
            system_prompt=system_prompt
        )
        body["stream"] = True
        
        try:
            endpoint = f"{self.base_url}/messages"
            async with self._client.stream("POST", endpoint, json=body) as response:
                response.raise_for_status()
                async for line in response.aiter_lines():
                    if line.startswith("data: "):
                        data_str = line[6:]
                        if data_str == "[DONE]":
                            break
                        try:
                            data = json.loads(data_str)
                            if "delta" in data:
                                text = data["delta"].get("text", "")
                                if text:
                                    yield text
                        except json.JSONDecodeError:
                            continue
        except Exception as e:
            console.print(f"[yellow]Streaming error, falling back to offline mode[/yellow]")
            yield self._generate_offline_analysis(prompt, context)
    
    def _generate_offline_analysis(
        self,
        prompt: str,
        context: Optional[Dict[str, Any]] = None
    ) -> str:
        """Generate offline analysis when API is unavailable"""
        analysis = """
## AI Analysis (Offline Mode)

**Note:** This analysis was generated in offline mode due to API unavailability.
For comprehensive AI-powered analysis, please configure your API key.

### General Security Recommendations:

1. **Input Validation**: Ensure all user inputs are properly validated and sanitized
2. **Authentication**: Implement strong authentication mechanisms
3. **Authorization**: Apply principle of least privilege
4. **Encryption**: Use TLS 1.3 for data in transit, AES-256 for data at rest
5. **Logging**: Maintain comprehensive security logs
6. **Updates**: Keep all systems and dependencies updated

### Scan Context Summary:
"""
        if context:
            for key, value in context.items():
                if isinstance(value, dict):
                    analysis += f"\n- **{key}**: {len(value)} items found"
                elif isinstance(value, list):
                    analysis += f"\n- **{key}**: {len(value)} entries"
                else:
                    analysis += f"\n- **{key}**: {value}"
        
        return analysis
    
    def clear_history(self):
        """Clear conversation history"""
        self.conversation_history = []
    
    async def analyze_vulnerability(
        self,
        vulnerability_type: str,
        evidence: Dict[str, Any],
        target: str,
    ) -> AIResponse:
        """
        Analyze a specific vulnerability finding
        
        Args:
            vulnerability_type: Type of vulnerability (e.g., XSS, SQLi)
            evidence: Evidence data from scanning
            target: Target URL or host
        
        Returns:
            Detailed vulnerability analysis
        """
        from .prompts import PentestPrompts
        
        prompt = PentestPrompts.vulnerability_analysis(
            vuln_type=vulnerability_type,
            evidence=evidence,
            target=target
        )
        
        return await self.analyze(
            prompt=prompt,
            system_prompt=PentestPrompts.SECURITY_ANALYST_SYSTEM,
            context=evidence
        )
    
    async def generate_report_section(
        self,
        section: str,
        findings: List[Dict[str, Any]],
        target_info: Dict[str, Any],
    ) -> AIResponse:
        """
        Generate a section of the pentest report
        
        Args:
            section: Section name (executive_summary, technical_findings, etc.)
            findings: List of vulnerability findings
            target_info: Information about the target
        
        Returns:
            Generated report section
        """
        from .prompts import PentestPrompts
        
        prompt = PentestPrompts.report_section(
            section=section,
            findings=findings,
            target_info=target_info
        )
        
        return await self.analyze(
            prompt=prompt,
            system_prompt=PentestPrompts.REPORT_WRITER_SYSTEM,
            context={"findings": findings, "target": target_info}
        )
